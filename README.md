# Mark Interview

## Prerequisites

> Install and Run Ollama locally

- [Download Ollama](https://ollama.ai/download) and install it locally.
- run ollama run mistral to download and install the model locally (Requires 4.1GB and 8GB of RAM).
- Open [http://localhost:11434](http://localhost:11434/) to check if Ollama is running.

## Local development

- update **.env.local** to set Ollama configuration.
- `pnpm install` to install the required dependencies.
- `pnpm dev` to launch the development server.
- Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.
